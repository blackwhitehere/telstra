{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time as t\n",
    "from sklearn import preprocessing as pre\n",
    "\n",
    "path=\"C:\\\\dev\\\\telstra\\\\data\\\\\"\n",
    "\n",
    "trainFile=\"train.csv\"\n",
    "testFile=\"test.csv\"\n",
    "resourceFile=\"resource_type.csv\"\n",
    "eventTypeFile=\"event_type.csv\"\n",
    "logFeatureFile=\"log_feature.csv\"\n",
    "severityTypeFile=\"severity_type.csv\"\n",
    "\n",
    "test=pd.read_csv(filepath_or_buffer=path+testFile,delimiter=\",\",header=0)\n",
    "train=pd.read_csv(filepath_or_buffer=path+trainFile,delimiter=\",\",header=0)\n",
    "\n",
    "resource=pd.read_csv(filepath_or_buffer=path+resourceFile,delimiter=\",\",header=0)\n",
    "event=pd.read_csv(filepath_or_buffer=path+eventTypeFile,delimiter=\",\",header=0)\n",
    "feature=pd.read_csv(filepath_or_buffer=path+logFeatureFile,delimiter=\",\",header=0)\n",
    "severity=pd.read_csv(filepath_or_buffer=path+severityTypeFile,delimiter=\",\",header=0)\n",
    "join=pd.DataFrame({'id':[]})\n",
    "datasets={'train':train,'test':test,'resource':resource,'event':event,'feature':feature,'severity':severity}\n",
    "notCat=['id','volume']\n",
    "\n",
    "#convert table of ids and classes to binary row feature vectors for each unique id\n",
    "#takes a long time, thus once computed save results to csv file\n",
    "for key, dataset in datasets.items():\n",
    "    tmp=pd.get_dummies(dataset,dummy_na=True)\n",
    "    tmp=tmp.groupby('id').sum()\n",
    "    tmp['id']=tmp.index\n",
    "    dataset=tmp\n",
    "    datasets[key]=dataset\n",
    "    \n",
    "#creates join dataset table that will store features for train and test datasets\n",
    "datasets['train']['sample']='train'\n",
    "datasets['test']['sample']='test'\n",
    "datasets['test']['fault_severity']=np.nan\n",
    "join=pd.concat([datasets['train'],datasets['test']],ignore_index=True)\n",
    "\n",
    "#joins features from different datasets into one dataframe, features only for evaluated ids are extracted\n",
    "#(i.e. some features may be not related to any id)\n",
    "samples=['train','test']\n",
    "for key, dataset in datasets.items():\n",
    "    if key not in samples:\n",
    "        join=pd.merge(join,dataset,on='id',how='left') #can use concat?\n",
    "\n",
    "join=join.set_index(['sample',join.index])\n",
    "\n",
    "#normalizes a continuous feature\n",
    "join['volume']=pre.scale(join['volume'])\n",
    "\n",
    "#fills in missing data\n",
    "from sklearn.base import TransformerMixin\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].median() for c in X],\n",
    "            index=X.columns)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "    \n",
    "join = DataFrameImputer().fit_transform(join)\n",
    "#join=join.fillna(value=0)\n",
    "\n",
    "#creates features and targets for final training\n",
    "features=join.columns.values\n",
    "features=features[2:]\n",
    "\n",
    "train_features = join.loc['train'][features]\n",
    "train_target = join.loc['train']['fault_severity']\n",
    "\n",
    "test_features = join.loc['test'][features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import RidgeClassifierCV, Perceptron, SGDClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import log_loss, accuracy_score, classification_report\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier, OutputCodeClassifier\n",
    "from sklearn import grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(train_features, train_target, test_size=0.31, random_state=0)\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = t.time()\n",
    "    #clf=OneVsRestClassifier(clf) #may want to remove for testing\n",
    "    clf.fit(train_x, train_y)\n",
    "    train_time = t.time() - t0\n",
    "    print(\"Training time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = t.time()\n",
    "    y_pred_single = clf.predict(test_x)\n",
    "    if (hasattr(clf,'predict_proba')):\n",
    "        y_pred_dist = clf.predict_proba(test_x)\n",
    "        logloss=log_loss(test_y,y_pred_dist)\n",
    "        print(\"LogLoss: %0.3fs\" % logloss)\n",
    "    test_time = t.time() - t0\n",
    "    print(\"Prediction time:  %0.3fs\" % test_time)\n",
    "\n",
    "    acc_score = accuracy_score(test_y, y_pred_single)\n",
    "    print(\"Accuracy:   %0.3f\" % acc_score)\n",
    "    \n",
    "    #if (hasattr(clf,'predict_proba')):\n",
    "       #scores = cross_val_score(clf,train_features,train_target, scoring='log_loss')\n",
    "        #loss_score=scores.mean()\n",
    "        #print(\"CV LogLoss: %0.2f (+/- %0.2f)\" % (loss_score, scores.std() * 2))\n",
    "    \n",
    "    print(classification_report(test_y, y_pred_single))\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    \n",
    "    if (hasattr(clf,'predict_proba')):\n",
    "        return clf_descr, acc_score, logloss, train_time, test_time\n",
    "    else:\n",
    "        return clf_descr, acc_score, train_time, test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifierCV(alphas=(0.1, 1.0, 10.0), class_weight=None, cv=None,\n",
      "         fit_intercept=True, normalize=False, scoring=None)\n",
      "Training time: 4.643s\n",
      "Prediction time:  0.038s\n",
      "Accuracy:   0.727\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.79      0.91      0.84      1476\n",
      "        1.0       0.59      0.35      0.44       593\n",
      "        2.0       0.49      0.55      0.52       220\n",
      "\n",
      "avg / total       0.71      0.73      0.71      2289\n",
      "\n",
      "================================================================================\n",
      "Random forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Training time: 17.260s\n",
      "Prediction time:  0.930s\n",
      "Accuracy:   0.736\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.88      0.84      1476\n",
      "        1.0       0.59      0.47      0.52       593\n",
      "        2.0       0.54      0.49      0.51       220\n",
      "\n",
      "avg / total       0.72      0.74      0.73      2289\n",
      "\n",
      "================================================================================\n",
      "AdaBoost\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "Training time: 2.913s\n",
      "Prediction time:  0.465s\n",
      "Accuracy:   0.720\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.77      0.90      0.83      1476\n",
      "        1.0       0.58      0.31      0.40       593\n",
      "        2.0       0.54      0.62      0.58       220\n",
      "\n",
      "avg / total       0.70      0.72      0.70      2289\n",
      "\n",
      "================================================================================\n",
      "Bagging\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False)\n",
      "Training time: 3.196s\n",
      "Prediction time:  0.596s\n",
      "Accuracy:   0.730\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.79      0.87      0.83      1476\n",
      "        1.0       0.59      0.47      0.52       593\n",
      "        2.0       0.55      0.49      0.52       220\n",
      "\n",
      "avg / total       0.72      0.73      0.72      2289\n",
      "\n",
      "================================================================================\n",
      "DiscriminatnAnalysis\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,\n",
      "               store_covariances=False, tol=0.0001)\n",
      "Training time: 5.013s\n",
      "Prediction time:  1.510s\n",
      "Accuracy:   0.603\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.71      0.76      0.73      1476\n",
      "        1.0       0.44      0.40      0.42       593\n",
      "        2.0       0.09      0.07      0.08       220\n",
      "\n",
      "avg / total       0.58      0.60      0.59      2289\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:688: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for clf, name in (\n",
    "        (RidgeClassifierCV(), \"Ridge Classifier\"),\n",
    "        (RandomForestClassifier(n_estimators=300), \"Random forest\"),\n",
    "        (AdaBoostClassifier(), \"AdaBoost\"),\n",
    "        (BaggingClassifier(), \"Bagging\"),\n",
    "        (QuadraticDiscriminantAnalysis(), \"DiscriminatnAnalysis\")):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=7, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=200, presort=True,\n",
      "              random_state=None, subsample=1.0, verbose=1,\n",
      "              warm_start=False),\n",
      "          n_jobs=-2)\n",
      "Training time: 92.315s\n",
      "Prediction time:  0.573s\n",
      "Accuracy:   0.739\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.89      0.84      1476\n",
      "        1.0       0.61      0.42      0.50       593\n",
      "        2.0       0.55      0.60      0.58       220\n",
      "\n",
      "avg / total       0.73      0.74      0.73      2289\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('OneVsRestClassifier',\n",
       " 0.73918741808650068,\n",
       " 0.57045593736126277,\n",
       " 92.31480956077576,\n",
       " 0.5728719234466553)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test a classifier\n",
    "\n",
    "# Fit the model to training data\n",
    "#parameters={'n_estimators':[100,150]},'max_depth':[3]}\n",
    "clf = OneVsRestClassifier(GradientBoostingClassifier(n_estimators=200,verbose=1,presort=True,max_depth=7),n_jobs=-2)#,min_samples_leaf=10,min_samples_split=5)\n",
    "#clf=grid_search.GridSearchCV(gbc, parameters,n_jobs=-2)\n",
    "benchmark(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=10,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=-2,\n",
      "            oob_score=True, random_state=None, verbose=1, warm_start=False),\n",
      "          n_jobs=-2)\n",
      "Training time: 31.099s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 400 out of 400 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 400 out of 400 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 400 out of 400 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 400 out of 400 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 400 out of 400 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 400 out of 400 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction time:  1.980s\n",
      "Accuracy:   0.751\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.90      0.85      1476\n",
      "        1.0       0.63      0.44      0.52       593\n",
      "        2.0       0.59      0.56      0.57       220\n",
      "\n",
      "avg / total       0.74      0.75      0.74      2289\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('OneVsRestClassifier',\n",
       " 0.75141983398864132,\n",
       " 0.54245802948043198,\n",
       " 31.099143743515015,\n",
       " 1.9801154136657715)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test a classifier\n",
    "\n",
    "# Fit the model to training data\n",
    "#parameters={'n_estimators':[100,150]},'max_depth':[3]}\n",
    "clf = OneVsRestClassifier(RandomForestClassifier(n_estimators=400,verbose=1,min_samples_split=10,\\\n",
    "                                                 oob_score=True,n_jobs=-2),n_jobs=-2)\n",
    "#clf=grid_search.GridSearchCV(gbc, parameters,n_jobs=-2)\n",
    "benchmark(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLoss: -0.56 (+/- 0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done   3 out of   3 | elapsed:   44.6s finished\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf,train_features,train_target, scoring='log_loss',n_jobs=-2,verbose=True)\n",
    "print(\"LogLoss: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestCLF=clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-2)]: Done 194 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-2)]: Done 400 out of 400 | elapsed:   10.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=800,\n",
       "            min_samples_leaf=1, min_samples_split=10,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=-2,\n",
       "            oob_score=True, random_state=None, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestCLF.fit(train_features,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 400 out of 400 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "test_response=bestCLF.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save predictions to required format \n",
    "#make it binary?\n",
    "out=pd.DataFrame({'id':[],'predict_0':[],'predict_1':[],'predict_2':[]})\n",
    "out['id']=test['id']\n",
    "out['predict_0']=test_response.T[0]\n",
    "out['predict_1']=test_response.T[1]\n",
    "out['predict_2']=test_response.T[2]\n",
    "out=out[['id','predict_0','predict_1','predict_2']]\n",
    "out.to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>fault_severity</th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>event_type</th>\n",
       "      <th>severity_type</th>\n",
       "      <th>resource_type</th>\n",
       "      <th>log_feature</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">train</th>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14121</td>\n",
       "      <td>148</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>237</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14121</td>\n",
       "      <td>148</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>14121</td>\n",
       "      <td>148</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>237</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14121</td>\n",
       "      <td>148</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>9320</td>\n",
       "      <td>1027</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>240</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fault_severity     id  location  event_type  severity_type  \\\n",
       "sample                                                                 \n",
       "train  0               1  14121       148          26              1   \n",
       "       1               1  14121       148          26              1   \n",
       "       2               1  14121       148          27              1   \n",
       "       3               1  14121       148          27              1   \n",
       "       4               0   9320      1027          26              1   \n",
       "\n",
       "          resource_type  log_feature  volume  \n",
       "sample                                        \n",
       "train  0              2          237      19  \n",
       "       1              2          148      19  \n",
       "       2              2          237      19  \n",
       "       3              2          148      19  \n",
       "       4              2          240     200  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transform cat labels into digits\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "for column in join.columns:\n",
    "    if column not in notCat:\n",
    "        join[column]=le.fit_transform(join[column])\n",
    "    \n",
    "join.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
