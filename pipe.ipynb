{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path=\"C:\\\\Dropbox\\\\Google\\\\MSc BA\\\\Courses\\\\AML\\\\Kaggle\\\\Telstra\\\\Data\\\\\"\n",
    "path=\"C:\\\\Users\\\\stan\\\\Google Drive\\\\MSc BA\\\\Courses\\\\AML\\\\Kaggle\\\\Telstra\\\\Data\\\\\"\n",
    "\n",
    "trainFile=\"train.csv\"\n",
    "testFile=\"test.csv\"\n",
    "resourceFile=\"resource_type.csv\"\n",
    "eventTypeFile=\"event_type.csv\"\n",
    "logFeatureFile=\"log_feature.csv\"\n",
    "severityTypeFile=\"severity_type.csv\"\n",
    "\n",
    "test=pd.read_csv(filepath_or_buffer=path+testFile,delimiter=\",\",header=0)\n",
    "train=pd.read_csv(filepath_or_buffer=path+trainFile,delimiter=\",\",header=0)\n",
    "\n",
    "resource=pd.read_csv(filepath_or_buffer=path+resourceFile,delimiter=\",\",header=0)\n",
    "event=pd.read_csv(filepath_or_buffer=path+eventTypeFile,delimiter=\",\",header=0)\n",
    "feature=pd.read_csv(filepath_or_buffer=path+logFeatureFile,delimiter=\",\",header=0)\n",
    "severity=pd.read_csv(filepath_or_buffer=path+severityTypeFile,delimiter=\",\",header=0)\n",
    "join=pd.DataFrame({'id':[]})\n",
    "features={'event':event,'feature':feature,'severity':severity} #add 'resource':resource back\n",
    "notCat=['id','volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert table of ids and classes to binary row feature vectors for each unique id\n",
    "#takes a long time, thus once computed save results to csv file\n",
    "for key, feature in features.items():\n",
    "    for column in feature.columns:\n",
    "        if column not in notCat:\n",
    "            tmp=pd.get_dummies(feature[column],sparse=True)\n",
    "            tmp['id']=feature['id']\n",
    "            tmp=tmp.groupby('id').sum()\n",
    "            feature=tmp\n",
    "            features[key]=feature\n",
    "            filename='new '+str(key)+'.csv'\n",
    "            feature.to_csv(filename)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates join dataset table that will store features for train and test datasets\n",
    "train['sample']='train'\n",
    "test['sample']='test'\n",
    "test['fault_severity']=np.nan\n",
    "join=pd.concat([train,test],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     1,     2, ..., 18550, 18551, 18552], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#joins features from different datasets into one dataframe, features only for evaluated ids are extracted\n",
    "#(i.e. some features may be not related to any id)\n",
    "for key,dataset in features.items():\n",
    "        join=pd.merge(join,dataset,on='id',how='left')\n",
    "\n",
    "join=join.set_index(['sample',join.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#retired\n",
    "import collections\n",
    "cat=collections.namedtuple('CategoricalConverter',['map','numSeries'])\n",
    "    \n",
    "def catSeries2numSeries(catSeries):\n",
    "    cats=np.sort(catSeries.unique())\n",
    "    cat_mapping=dict(zip(cats,range(0,len(cats)+1)))\n",
    "    numSeries=catSeries.map(cat_mapping).astype(int)\n",
    "    obs=cat(cat_mapping,numSeries)\n",
    "    return obs\n",
    "\n",
    "catLabels=join.dtypes[join.dtypes.map(lambda x: x == 'object')]\n",
    "catLabels=catLabels.index\n",
    "cat_map={}\n",
    "counter=0\n",
    "for label in catLabels:\n",
    "    convert=catSeries2numSeries(join[label])\n",
    "    cat_map[label]=convert.map #attribute from CategoricalConverter\n",
    "    join[label]=convert.numSeries\n",
    "    #setting categorical mapping; this is bogus, since vars are categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "features=['location','log_feature','severity_type','event_type', 'resource_type','volume']\n",
    "\n",
    "train_features = join.loc['train'][features]\n",
    "train_target = join.loc['train']['fault_severity']\n",
    "\n",
    "test_features = join.loc['test'][features]\n",
    "\n",
    "# Fit the model to training data\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf = clf.fit(train_features, train_target)\n",
    "\n",
    "score = clf.score(train_features, train_target)\n",
    "\"Mean accuracy of Random Forest: {0}\".format(score)\n",
    "\n",
    "test_response=clf.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split 80-20 train vs test data\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_features, train_target, test_size=0.31, random_state=0)\n",
    "\n",
    "clf = clf.fit(train_x, train_y)\n",
    "predict_y = clf.predict_proba(test_x)\n",
    "\n",
    "print (\"LogLoss = %.2f\" % (log_loss(test_y, predict_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save predictions to required format\n",
    "out=pd.DataFrame({'id':[],'predict_0':[],'predict_1':[],'predict_2':[]})\n",
    "out['id']=test['id']\n",
    "out['predict_0']=test_response.T[0]\n",
    "out['predict_1']=test_response.T[1]\n",
    "out['predict_2']=test_response.T[2]\n",
    "out=out[['id','predict_0','predict_1','predict_2']]\n",
    "meanOut.to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
